{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080bb9fb",
   "metadata": {},
   "source": [
    "README INFO: What is this pipeline, what does it do, what are its expected results?\n",
    "\n",
    "What is this pipeline?\n",
    "This pipeline can be divided into two parts: data acquisition/pre-processing and data syncing. If a user already has directories with the images they'd like to sync, and if the image names are in a format the syncing pipeline can understand, the user can skip the first part of the pipeline and go straight to the data syncing. It should also be noted that both parts of the pipeline can be performed separately and at separate times. A user can also run the first part of the pipeline multiple times before running the second part. The purpose of this pipeline is to create a database of machine-learning-ready solar images that can immediately be used in modeling. It serves as a data pre-processing step in the machine-learning modeling process. \n",
    "\n",
    "What does this pipeline do?\n",
    "Initially, the pipeline will grab user-specified SDO or SOHO solar data images for user-specified dates at a user-specified cadence. Any images with missing data or planetary transits (LASCO-specific) will be replaced, if possible, with a clean image at a similar datetime. The goal is to create a set of useful images at more-or-less even time steps (one image at each time step). Then part II of the pipeline can take several differnt groups of images (e.g. multiple runs of part I of the pipeline) and sync them so that for each datetime, the resulting directory will contain one image from each product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b278d3a",
   "metadata": {},
   "source": [
    "PIPELINE: PART I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f618c",
   "metadata": {},
   "source": [
    "First, import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0890def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from dateutil import parser\n",
    "from datetime import timedelta\n",
    "from sunpy.time import TimeRange\n",
    "from time import process_time\n",
    "from tqdm import tqdm\n",
    "import drms\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "import Mission_utility.data_gen_helper as ipy\n",
    "import Mission_utility.sdo_mdi as sdo\n",
    "import Mission_utility.soho_other as soho\n",
    "from Mission_utility.product_time_sync import csv_times_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12931fa2",
   "metadata": {},
   "source": [
    "Next, define all preliminary initial variables and perform any basic calculations/transforms to prepare initial variables that will be used in the workhorse for-loop. The next two code cells can be skipped if you already have the data you want and just need to sync data.\n",
    "\n",
    "EXPLAIN VARIABLES (here or comments?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d78463d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start =  '2001-04-01'#'2002-04-01' #'1999-04-04' # '2010-10-01' '2005-12-01'\n",
    "date_finish = '2001-04-07'#'2002-04-07' #'1999-04-09' # '2010-10-05' '2005-12-07'\n",
    "image_size_output = 128\n",
    "time_window = 6\n",
    "flag = 'subsample'\n",
    "home_dir = '/Users/gohawks/Desktop/swpc-local/soho-ml-data/soho-ml-data-ready-martinkus/'\n",
    "bases = 'MDI_96m' #,LASCO_C3'#MDI_96m #AIA #HMI #'EIT195'\n",
    "fits_headers = 'N'\n",
    "lev1_LASCO = 'Y' #CANNOT USE 'N' UNTIL UNIT CONVERSION SORTED OUT\n",
    "email = 'charlotte.martinkus@noaa.gov'\n",
    "\n",
    "client = drms.Client(email=email, verbose=False) \n",
    "date_time_pre_start = date_start + '-0000'\n",
    "date_time_start= parser.parse(date_time_pre_start)\n",
    "date_time_pre_end = date_finish + '-2359'\n",
    "date_time_end = parser.parse(date_time_pre_end)\n",
    "time_increment = 60\n",
    "url_prefix = 'https://seal.nascom.nasa.gov'\n",
    "look_ahead = int(np.ceil(time_window*60/10.)) #should sufficiently cover all 7 products based on their cadence.\n",
    "diff_start_finish_total_sec = (date_time_end - date_time_start).total_seconds()\n",
    "total_sec = timedelta(days = time_increment).total_seconds()\n",
    "num_loops = np.ceil(diff_start_finish_total_sec/total_sec) + 1 #num_loops would be equal to 94 + 1 for 19960101-0000' - '20110501-0000'\n",
    "base_list = bases.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de94397",
   "metadata": {},
   "source": [
    "Begin the main workhorse loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for base in tqdm(base_list):\n",
    "        start_process_time = process_time() #initialize clock per product type\n",
    "            \n",
    "        base = base.strip(' ')\n",
    "        if ('LASCO' in base) or ('EIT' in base):\n",
    "            BaseClass = soho.SOHO_no_MDI(base, lev1_LASCO, fits_headers)\n",
    "            \n",
    "        else:\n",
    "            BaseClass = sdo.SDO_MDI(base, lev1_LASCO, fits_headers, time_window)\n",
    "            \n",
    "        BaseClass.set_base_dictionary()\n",
    "        \n",
    "        holes_list = []\n",
    "        transients_list = []\n",
    "        unreadable_file_ids_product_list_global = []\n",
    "    \n",
    "        print(f'{base}')\n",
    "        base_dir = home_dir + base + '_' + BaseClass.mission\n",
    "        if not os.path.exists(base_dir):\n",
    "            os.makedirs(base_dir)   \n",
    "            \n",
    "        time_range = TimeRange(date_time_start, timedelta(days = time_increment))\n",
    "        \n",
    "        \n",
    "        prev_time, time_range_modified = ipy.prev_time_resumer(home_dir, time_range, date_time_end, BaseClass) #time_range_modified.next() is the workshorse that advances time at the end of the time for-loop\n",
    "        for t_value in tqdm(np.arange(num_loops)): #this is the main workhorse loop of the program\n",
    "            #print('t_value:', t_value)\n",
    "            print('prev_time:', prev_time)\n",
    "                \n",
    "            if time_range_modified.end > date_time_end:\n",
    "                time_range_modified = TimeRange(time_range_modified.start, date_time_end) \n",
    "                \n",
    "            product_results, client = BaseClass.product_search(time_range_modified, client)\n",
    "            \n",
    "            if BaseClass.class_type == 'SDO_MDI':\n",
    "                client_export_failed = product_results.has_failed()\n",
    "                \n",
    "                if client_export_failed == True:\n",
    "                    product_results_number = 0\n",
    "                \n",
    "                elif client_export_failed == False:\n",
    "                    product_results_number = product_results.data.count()['record'] \n",
    "            \n",
    "            else:\n",
    "                product_results_number = product_results.file_num\n",
    "                client_export_failed = False\n",
    "                \n",
    "            if (product_results_number != 0) and (client_export_failed == False): #product_results.has_failed()\n",
    "                ind = BaseClass.index_of_sizes(product_results) \n",
    "                size_sieved_df, fetch_indices_product_orig = ipy.fetch_indices(ind,product_results,time_window, prev_time, time_range_modified, BaseClass)\n",
    "                if len(fetch_indices_product_orig) != 0:\n",
    "                \n",
    "                    size_sieved_df = ipy.product_distiller(fetch_indices_product_orig, size_sieved_df, date_time_end, product_results, look_ahead, time_window, url_prefix, flag, image_size_output, home_dir, email, client, BaseClass)\n",
    "                    \n",
    "                    all_time_window_sieved_times_sorted = np.unique(list(size_sieved_df['time_at_ind']))\n",
    "                                \n",
    "                    prev_time = [] #reset to empty list\n",
    "                    if len(all_time_window_sieved_times_sorted) != 0:\n",
    "                        prev_time.append(all_time_window_sieved_times_sorted[-1]) #append the last good time entry from the previous loop     \n",
    "                    size_sieved_df.to_csv(f'{home_dir}{date_start}_to_{date_finish}_{BaseClass.base_full}_times_{flag}_{time_window}_LASCOlev1-{BaseClass.lev1_lasco}_{BaseClass.mission}_{image_size_output}_{t_value}.csv')\n",
    "                        \n",
    "                        #ipy.csv_writer(home_dir, date_start, date_finish, flag, time_window, image_size_output, all_time_window_sieved_times_sorted, BaseClass)\n",
    "                    \n",
    "    \n",
    "                time_range_modified.next() #Sunpy iterator to go for the next time increment in number of days. There is also time_range_modified.previous() to go backwards in time.    \n",
    "                #print('time_range_modified next:', time_range_modified)\n",
    "            \n",
    "\n",
    "\n",
    "        ipy.data_cuber(home_dir, date_start, date_finish, flag, time_window, image_size_output,BaseClass)\n",
    "        \n",
    "        end_process_time = process_time()\n",
    "        time_of_process = end_process_time - start_process_time\n",
    "        print(f'{base} time of process in seconds:', time_of_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f56cb",
   "metadata": {},
   "source": [
    "PIPELINE: PART II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe2d9c",
   "metadata": {},
   "source": [
    "Next in the process is syncing any data you want to work with. (retroactive metadata seeding???)\n",
    "First, define initial variables.\n",
    "\n",
    "EXPLAIN VARIABLES (here or comments?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start =  '2005-12-01'#'2002-04-01' #'1999-04-04' # '2010-10-01'\n",
    "date_finish = '2005-12-07'#'2002-04-07' #'1999-04-09' # '2010-10-05'\n",
    "time_step = 12\n",
    "home_dir = '/Users/gohawks/Desktop/swpc-local/soho-ml-data/soho-ml-data-ready-martinkus/'\n",
    "bases = 'LASCO_C2, EIT195' #,LASCO_C3'#MDI_96m #AIA #HMI #'EIT195'\n",
    "fits_provided = 'Y' #are fits files provided?\n",
    "sub_Fcorona = 'N' #if applicable, do you want additional data cubes that subtract fcorona?\n",
    "\n",
    "start_process_time = process_time() #initialize clock\n",
    "\n",
    "product_list = [] #list for all products #will need to hsplit this by base_list_len - won't result in equal division: need another approach\n",
    "slice_start_ind_list = [] #to keep track of the corresponding subset of cube slice indices if the original corresponding time range has been modified.\n",
    "slice_end_ind_list = []\n",
    "BaseClass_list = []\n",
    "\n",
    "base_list = bases.split(',')\n",
    "base_list_len = len(base_list)\n",
    "time_step_sec = time_step*3600\n",
    "    \n",
    "data_dim_list = []\n",
    "time_step_prev_list = []\n",
    "dim_err = \"there is a mismatch in dimensionality among one or more products (e.g., 128x128 vs 256x256) in the directory\"\n",
    "time_step_err = \"selected time step has to be greater than or equal to the original time step used (e.g., can not choose < 6 hrs if original time step was 6 hrs)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f34eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for base in base_list:\n",
    "    \n",
    "    base = base.strip(' ')\n",
    "\n",
    "    BaseClass = pts.Base_Class(base, home_dir, time_step, date_start, date_finish)\n",
    "        \n",
    "    BaseClass.set_base_dictionary()\n",
    "    BaseClass_list.append(BaseClass)\n",
    "    \n",
    "    time_step_prev = BaseClass.time_step_prev_reader()\n",
    "    time_step_prev_list.append(time_step_prev)\n",
    "    \n",
    "    if (time_step_prev > time_step):\n",
    "        raise ValueError(time_step_err)\n",
    "        exit\n",
    "    \n",
    "    print('time_step_prev:', time_step_prev)\n",
    "    \n",
    "    if (fits_provided.upper() == 'Y'): #so deal with .fits files since contain all info that need such as the time and dim. additionally have h5 cube and csv file too.\n",
    "        found_times = BaseClass.fits_times_reader()\n",
    "        data_dim_list = BaseClass.dimension_checker_from_fits(data_dim_list)\n",
    "        \n",
    "    elif (fits_provided.upper() == 'N'): #so dealing with the downloaded datasync_times_and_inds cubes and csv files but no fits files. so need times from csv and dim from h5 cube or csv. both h5 and csv used.\n",
    "        found_times = BaseClass.csv_times_reader()\n",
    "        data_dim_list = BaseClass.dimension_checker_from_h5cube_csv(data_dim_list)\n",
    "        \n",
    "    product_list.append(pts.times_actualizer(found_times, BaseClass.date_start, BaseClass.date_finish)[0]) #so the first return of times_actualizer which now returns two objects\n",
    "    slice_start_ind_list.append(pts.times_actualizer(found_times, BaseClass.date_start, BaseClass.date_finish)[2][0]) #first start entry\n",
    "    slice_end_ind_list.append(pts.times_actualizer(found_times, BaseClass.date_start, BaseClass.date_finish)[3][-1]) #last finish entry\n",
    "                    \n",
    "    min_time_diff = pts.min_time_step(pts.times_actualizer(found_times, BaseClass.date_start, BaseClass.date_finish)[1])\n",
    "    print('min_time_diff in hours:', min_time_diff)\n",
    "    \n",
    "ind_dim = np.where(data_dim_list[0] == np.array(data_dim_list))[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ada5d",
   "metadata": {},
   "source": [
    "(what is this code??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ff77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ind_dim) != len(data_dim_list):   \n",
    "    raise ValueError(dim_err)\n",
    "    \n",
    "\n",
    "\n",
    "ind_min_len = pts.shortest_prod_list_index_finder(product_list)\n",
    "time_step_prev_max = np.max(time_step_prev_list)\n",
    "    \n",
    "sync_time_inds_list, sync_time_list = pts.sync_times_and_inds(product_list, ind_min_len, time_step, time_step_prev_max)\n",
    "sync_time_inds_list_mod, sync_time_list_mod = pts.sync_times_and_inds_sort_by_product(sync_time_inds_list, sync_time_list)\n",
    "#print('sync_time_list_mod_main:', sync_time_list_mod)\n",
    "\n",
    "BaseClass_list_len = len(BaseClass_list)\n",
    "\n",
    "lasco_diff_ind_Fcorona_24h_list = []\n",
    "lasco_diff_len_ind_Fcorona_24h_list = [] #C2 and C3 list should be the same size even after 24 hr index but good to check just in case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,BaseClass in tqdm(enumerate(BaseClass_list)):\n",
    "    \n",
    "    cube_data, cube_dim, meta_items  = BaseClass.cube_data_reader() #, cube_hdr #, meta_items\n",
    "    \n",
    "    if len(cube_data) == 0:\n",
    "        raise ValueError('No data cubes were found for the exact user-specified dates')\n",
    "    \n",
    "    BaseClass.cube_sync_maker(BaseClass_list_len, cube_data, cube_dim, meta_items, slice_start_ind_list[i], slice_end_ind_list[i], sync_time_inds_list_mod[i], time_step_prev_max) ###cube_dim, cube_hdr, ##### cube_dim, meta_items\n",
    "    BaseClass.csv_time_sync_writer(BaseClass_list_len, cube_dim, sync_time_list_mod[i], time_step_prev_max)\n",
    "    \n",
    "    if ('LASCO' in BaseClass.base_full) and (sub_Fcorona.upper() == 'Y'):\n",
    "        \n",
    "\n",
    "        \n",
    "        lasco_ind_Fcorona_24h = pts.lasco_diff_times_inds(sync_time_list_mod[i])\n",
    "        #print('lasco_ind_Fcorona_24h:',lasco_ind_Fcorona_24h)\n",
    "        \n",
    "        if len(lasco_ind_Fcorona_24h)>0:\n",
    "            lasco_diff_ind_Fcorona_24h_list.append(lasco_ind_Fcorona_24h)\n",
    "            lasco_diff_len_ind_Fcorona_24h_list.append(len(lasco_ind_Fcorona_24h))\n",
    "        \n",
    "            #print('lasco_diff_ind_Fcorona_24h_list:', lasco_diff_ind_Fcorona_24h_list)\n",
    "            print('lasco_diff_len_ind_Fcorona_24h_list:', lasco_diff_len_ind_Fcorona_24h_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(lasco_diff_ind_Fcorona_24h_list)!=0) and (sub_Fcorona.upper() == 'Y'):\n",
    "    \n",
    "    flag_lasco =  'Fcorona'   \n",
    "    \n",
    "    if (lasco_diff_len_ind_Fcorona_24h_list) and (len(np.unique(lasco_diff_len_ind_Fcorona_24h_list)) > 1):\n",
    "        ind_lasco_len_Fcorona_24h_min = np.where(np.array(lasco_diff_len_ind_Fcorona_24h_list) == np.min(lasco_diff_len_ind_Fcorona_24h_list))[0][0]\n",
    "        print('ind_lasco_len_Fcorona_24h_min:', ind_lasco_len_Fcorona_24h_min)\n",
    "        ind_lasco_principal_Fcorona_24h = lasco_diff_ind_Fcorona_24h_list[ind_lasco_len_Fcorona_24h_min]        \n",
    "    \n",
    "    elif (lasco_diff_len_ind_Fcorona_24h_list) and (len(np.unique(lasco_diff_len_ind_Fcorona_24h_list)) == 1):\n",
    "        ind_lasco_len_Fcorona_24h_min = 0\n",
    "        ind_lasco_principal_Fcorona_24h = lasco_diff_ind_Fcorona_24h_list[ind_lasco_len_Fcorona_24h_min]\n",
    "    \n",
    "    else:\n",
    "        ind_lasco_principal_Fcorona_24h = 0\n",
    "        \n",
    "    for i,BaseClass in tqdm(enumerate(BaseClass_list)): #loop to make time_diff set\n",
    "        \n",
    "        cube_data_pre, cube_dim, meta_items = BaseClass.cube_data_reader() #, cube_hdr #, meta_items\n",
    "        \n",
    "        if ('LASCO' in base):\n",
    "            cube_data_diff = cube_data_pre[1:] - cube_data_pre[:-1]\n",
    "            cube_data = cube_data_diff[ind_lasco_principal_Fcorona_24h] #don't need the +1 here as for the times since this is the difference cube\n",
    "        \n",
    "        else:\n",
    "            cube_data = cube_data_pre[ind_lasco_principal_Fcorona_24h+1]           \n",
    "        \n",
    "        BaseClass.cube_sync_maker(BaseClass_list_len, cube_data, cube_dim, meta_items, slice_start_ind_list[i], slice_end_ind_list[i], sync_time_inds_list_mod[i][ind_lasco_principal_Fcorona_24h+1], time_step_prev, flag_lasco) #adding one to get to original time value since was taking differce of the time array ###cube_dim, cube_hdr, #####cube_dim, meta_items\n",
    "        BaseClass.csv_time_sync_writer(BaseClass_list_len, cube_dim, sync_time_list_mod[i][ind_lasco_principal_Fcorona_24h+1], time_step_prev, flag_lasco)\n",
    "        #adding one to get to original time value since was taking differce of the time array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7dd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_process_time = process_time()\n",
    "time_of_process = end_process_time - start_process_time\n",
    "print('time to run in seconds:', time_of_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
